{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/sungeun/anaconda3/envs/pytorch10/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(16, 8)\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1          [-1, 1, 23343, 8]             136\n",
      "            Linear-2          [-1, 1, 23343, 4]              36\n",
      "================================================================\n",
      "Total params: 172\n",
      "Trainable params: 172\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.42\n",
      "Forward/backward pass size (MB): 2.14\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 3.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Regressor().to(device)\n",
    "summary(model, (1, 23343, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss =0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n",
    "                epoch, running_loss/log_interval))\n",
    "            running_loss =0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "log_interval =100\n",
    "\n",
    "train_dataset = pd.read_csv('/home/sungeun/deep-high-resolution-net.pytorch-master/output/train_data.csv')\n",
    "test_dataset = pd.read_csv('/home/sungeun/deep-high-resolution-net.pytorch-master/output/test_data.csv')\n",
    "\n",
    "# train, test / direction\n",
    "X_train = train_dataset.drop(['squat', 'direction'], axis=1)\n",
    "y_train = train_dataset['squat']\n",
    "X_val = test_dataset.drop(['squat', 'direction'], axis=1)\n",
    "y_val = test_dataset['squat']\n",
    "train_direction = train_dataset['direction']\n",
    "test_direction = test_dataset['direction']\n",
    "\n",
    "# dataframe -> numpy -> torch\n",
    "X_train = torch.from_numpy(X_train.values).float()\n",
    "y_train = torch.from_numpy(y_train.values).long()\n",
    "X_val = torch.from_numpy(X_val.values).float()\n",
    "y_val = torch.from_numpy(y_val.values).long()\n",
    "\n",
    "# train dataloader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 \tLoss: 0.014853\n",
      "Train Epoch: 2 \tLoss: 0.013433\n",
      "Train Epoch: 3 \tLoss: 0.011443\n",
      "Train Epoch: 4 \tLoss: 0.010801\n",
      "Train Epoch: 5 \tLoss: 0.010180\n",
      "Train Epoch: 6 \tLoss: 0.008922\n",
      "Train Epoch: 7 \tLoss: 0.008976\n",
      "Train Epoch: 8 \tLoss: 0.008350\n",
      "Train Epoch: 9 \tLoss: 0.008333\n",
      "Train Epoch: 10 \tLoss: 0.008652\n",
      "Train Epoch: 11 \tLoss: 0.008518\n",
      "Train Epoch: 12 \tLoss: 0.007763\n",
      "Train Epoch: 13 \tLoss: 0.008666\n",
      "Train Epoch: 14 \tLoss: 0.007691\n",
      "Train Epoch: 15 \tLoss: 0.007307\n",
      "Train Epoch: 16 \tLoss: 0.007136\n",
      "Train Epoch: 17 \tLoss: 0.006849\n",
      "Train Epoch: 18 \tLoss: 0.006561\n",
      "Train Epoch: 19 \tLoss: 0.006265\n",
      "Train Epoch: 20 \tLoss: 0.006519\n",
      "Train Epoch: 21 \tLoss: 0.005726\n",
      "Train Epoch: 22 \tLoss: 0.006844\n",
      "Train Epoch: 23 \tLoss: 0.005450\n",
      "Train Epoch: 24 \tLoss: 0.006218\n",
      "Train Epoch: 25 \tLoss: 0.005382\n",
      "Train Epoch: 26 \tLoss: 0.005652\n",
      "Train Epoch: 27 \tLoss: 0.005619\n",
      "Train Epoch: 28 \tLoss: 0.005508\n",
      "Train Epoch: 29 \tLoss: 0.005559\n",
      "Train Epoch: 30 \tLoss: 0.005029\n",
      "Train Epoch: 31 \tLoss: 0.005008\n",
      "Train Epoch: 32 \tLoss: 0.005147\n",
      "Train Epoch: 33 \tLoss: 0.004604\n",
      "Train Epoch: 34 \tLoss: 0.004652\n",
      "Train Epoch: 35 \tLoss: 0.004397\n",
      "Train Epoch: 36 \tLoss: 0.004976\n",
      "Train Epoch: 37 \tLoss: 0.004601\n",
      "Train Epoch: 38 \tLoss: 0.004718\n",
      "Train Epoch: 39 \tLoss: 0.004483\n",
      "Train Epoch: 40 \tLoss: 0.004549\n",
      "Train Epoch: 41 \tLoss: 0.004705\n",
      "Train Epoch: 42 \tLoss: 0.004433\n",
      "Train Epoch: 43 \tLoss: 0.004172\n",
      "Train Epoch: 44 \tLoss: 0.003679\n",
      "Train Epoch: 45 \tLoss: 0.004080\n",
      "Train Epoch: 46 \tLoss: 0.004443\n",
      "Train Epoch: 47 \tLoss: 0.003968\n",
      "Train Epoch: 48 \tLoss: 0.003676\n",
      "Train Epoch: 49 \tLoss: 0.003344\n",
      "Train Epoch: 50 \tLoss: 0.003484\n",
      "Train Epoch: 51 \tLoss: 0.003703\n",
      "Train Epoch: 52 \tLoss: 0.002906\n",
      "Train Epoch: 53 \tLoss: 0.003591\n",
      "Train Epoch: 54 \tLoss: 0.003514\n",
      "Train Epoch: 55 \tLoss: 0.003264\n",
      "Train Epoch: 56 \tLoss: 0.003889\n",
      "Train Epoch: 57 \tLoss: 0.003489\n",
      "Train Epoch: 58 \tLoss: 0.003646\n",
      "Train Epoch: 59 \tLoss: 0.003272\n",
      "Train Epoch: 60 \tLoss: 0.003193\n",
      "Train Epoch: 61 \tLoss: 0.003248\n",
      "Train Epoch: 62 \tLoss: 0.003046\n",
      "Train Epoch: 63 \tLoss: 0.003402\n",
      "Train Epoch: 64 \tLoss: 0.003076\n",
      "Train Epoch: 65 \tLoss: 0.003156\n",
      "Train Epoch: 66 \tLoss: 0.002994\n",
      "Train Epoch: 67 \tLoss: 0.003147\n",
      "Train Epoch: 68 \tLoss: 0.003114\n",
      "Train Epoch: 69 \tLoss: 0.002873\n",
      "Train Epoch: 70 \tLoss: 0.002675\n",
      "Train Epoch: 71 \tLoss: 0.002790\n",
      "Train Epoch: 72 \tLoss: 0.003085\n",
      "Train Epoch: 73 \tLoss: 0.003075\n",
      "Train Epoch: 74 \tLoss: 0.003435\n",
      "Train Epoch: 75 \tLoss: 0.002704\n",
      "Train Epoch: 76 \tLoss: 0.002647\n",
      "Train Epoch: 77 \tLoss: 0.002857\n",
      "Train Epoch: 78 \tLoss: 0.002495\n",
      "Train Epoch: 79 \tLoss: 0.002902\n",
      "Train Epoch: 80 \tLoss: 0.002912\n",
      "Train Epoch: 81 \tLoss: 0.002371\n",
      "Train Epoch: 82 \tLoss: 0.002527\n",
      "Train Epoch: 83 \tLoss: 0.002463\n",
      "Train Epoch: 84 \tLoss: 0.002798\n",
      "Train Epoch: 85 \tLoss: 0.002824\n",
      "Train Epoch: 86 \tLoss: 0.002826\n",
      "Train Epoch: 87 \tLoss: 0.002699\n",
      "Train Epoch: 88 \tLoss: 0.002410\n",
      "Train Epoch: 89 \tLoss: 0.002613\n",
      "Train Epoch: 90 \tLoss: 0.003022\n",
      "Train Epoch: 91 \tLoss: 0.002610\n",
      "Train Epoch: 92 \tLoss: 0.002350\n",
      "Train Epoch: 93 \tLoss: 0.002708\n",
      "Train Epoch: 94 \tLoss: 0.002380\n",
      "Train Epoch: 95 \tLoss: 0.002427\n",
      "Train Epoch: 96 \tLoss: 0.003084\n",
      "Train Epoch: 97 \tLoss: 0.002497\n",
      "Train Epoch: 98 \tLoss: 0.002264\n",
      "Train Epoch: 99 \tLoss: 0.002679\n",
      "Train Epoch: 100 \tLoss: 0.002227\n",
      "Train Epoch: 101 \tLoss: 0.002622\n",
      "Train Epoch: 102 \tLoss: 0.002342\n",
      "Train Epoch: 103 \tLoss: 0.002375\n",
      "Train Epoch: 104 \tLoss: 0.002323\n",
      "Train Epoch: 105 \tLoss: 0.002194\n",
      "Train Epoch: 106 \tLoss: 0.002157\n",
      "Train Epoch: 107 \tLoss: 0.001902\n",
      "Train Epoch: 108 \tLoss: 0.002340\n",
      "Train Epoch: 109 \tLoss: 0.002147\n",
      "Train Epoch: 110 \tLoss: 0.002330\n",
      "Train Epoch: 111 \tLoss: 0.002813\n",
      "Train Epoch: 112 \tLoss: 0.002549\n",
      "Train Epoch: 113 \tLoss: 0.002201\n",
      "Train Epoch: 114 \tLoss: 0.002295\n",
      "Train Epoch: 115 \tLoss: 0.002322\n",
      "Train Epoch: 116 \tLoss: 0.002394\n",
      "Train Epoch: 117 \tLoss: 0.002326\n",
      "Train Epoch: 118 \tLoss: 0.002292\n",
      "Train Epoch: 119 \tLoss: 0.002348\n",
      "Train Epoch: 120 \tLoss: 0.002334\n",
      "Train Epoch: 121 \tLoss: 0.002695\n",
      "Train Epoch: 122 \tLoss: 0.002411\n",
      "Train Epoch: 123 \tLoss: 0.002527\n",
      "Train Epoch: 124 \tLoss: 0.002607\n",
      "Train Epoch: 125 \tLoss: 0.002396\n",
      "Train Epoch: 126 \tLoss: 0.001822\n",
      "Train Epoch: 127 \tLoss: 0.002320\n",
      "Train Epoch: 128 \tLoss: 0.002315\n",
      "Train Epoch: 129 \tLoss: 0.002501\n",
      "Train Epoch: 130 \tLoss: 0.002597\n",
      "Train Epoch: 131 \tLoss: 0.002350\n",
      "Train Epoch: 132 \tLoss: 0.002155\n",
      "Train Epoch: 133 \tLoss: 0.001965\n",
      "Train Epoch: 134 \tLoss: 0.002074\n",
      "Train Epoch: 135 \tLoss: 0.002168\n",
      "Train Epoch: 136 \tLoss: 0.002513\n",
      "Train Epoch: 137 \tLoss: 0.002026\n",
      "Train Epoch: 138 \tLoss: 0.002375\n",
      "Train Epoch: 139 \tLoss: 0.002586\n",
      "Train Epoch: 140 \tLoss: 0.002055\n",
      "Train Epoch: 141 \tLoss: 0.002120\n",
      "Train Epoch: 142 \tLoss: 0.002221\n",
      "Train Epoch: 143 \tLoss: 0.002183\n",
      "Train Epoch: 144 \tLoss: 0.002005\n",
      "Train Epoch: 145 \tLoss: 0.001756\n",
      "Train Epoch: 146 \tLoss: 0.002430\n",
      "Train Epoch: 147 \tLoss: 0.002123\n",
      "Train Epoch: 148 \tLoss: 0.002517\n",
      "Train Epoch: 149 \tLoss: 0.002130\n",
      "Train Epoch: 150 \tLoss: 0.001802\n",
      "Train Epoch: 151 \tLoss: 0.002283\n",
      "Train Epoch: 152 \tLoss: 0.002786\n",
      "Train Epoch: 153 \tLoss: 0.002551\n",
      "Train Epoch: 154 \tLoss: 0.002004\n",
      "Train Epoch: 155 \tLoss: 0.001847\n",
      "Train Epoch: 156 \tLoss: 0.002310\n",
      "Train Epoch: 157 \tLoss: 0.002879\n",
      "Train Epoch: 158 \tLoss: 0.002335\n",
      "Train Epoch: 159 \tLoss: 0.002024\n",
      "Train Epoch: 160 \tLoss: 0.002058\n",
      "Train Epoch: 161 \tLoss: 0.002234\n",
      "Train Epoch: 162 \tLoss: 0.002156\n",
      "Train Epoch: 163 \tLoss: 0.001824\n",
      "Train Epoch: 164 \tLoss: 0.002179\n",
      "Train Epoch: 165 \tLoss: 0.001813\n",
      "Train Epoch: 166 \tLoss: 0.001984\n",
      "Train Epoch: 167 \tLoss: 0.002190\n",
      "Train Epoch: 168 \tLoss: 0.001856\n",
      "Train Epoch: 169 \tLoss: 0.002206\n",
      "Train Epoch: 170 \tLoss: 0.001817\n",
      "Train Epoch: 171 \tLoss: 0.003086\n",
      "Train Epoch: 172 \tLoss: 0.002122\n",
      "Train Epoch: 173 \tLoss: 0.002235\n",
      "Train Epoch: 174 \tLoss: 0.001819\n",
      "Train Epoch: 175 \tLoss: 0.001668\n",
      "Train Epoch: 176 \tLoss: 0.001907\n",
      "Train Epoch: 177 \tLoss: 0.001825\n",
      "Train Epoch: 178 \tLoss: 0.002492\n",
      "Train Epoch: 179 \tLoss: 0.002166\n",
      "Train Epoch: 180 \tLoss: 0.002151\n",
      "Train Epoch: 181 \tLoss: 0.002028\n",
      "Train Epoch: 182 \tLoss: 0.001848\n",
      "Train Epoch: 183 \tLoss: 0.001739\n",
      "Train Epoch: 184 \tLoss: 0.002494\n",
      "Train Epoch: 185 \tLoss: 0.001956\n",
      "Train Epoch: 186 \tLoss: 0.001705\n",
      "Train Epoch: 187 \tLoss: 0.002186\n",
      "Train Epoch: 188 \tLoss: 0.001748\n",
      "Train Epoch: 189 \tLoss: 0.001583\n",
      "Train Epoch: 190 \tLoss: 0.001872\n",
      "Train Epoch: 191 \tLoss: 0.001789\n",
      "Train Epoch: 192 \tLoss: 0.001711\n",
      "Train Epoch: 193 \tLoss: 0.001951\n",
      "Train Epoch: 194 \tLoss: 0.002150\n",
      "Train Epoch: 195 \tLoss: 0.002482\n",
      "Train Epoch: 196 \tLoss: 0.001866\n",
      "Train Epoch: 197 \tLoss: 0.001610\n",
      "Train Epoch: 198 \tLoss: 0.002270\n",
      "Train Epoch: 199 \tLoss: 0.002076\n",
      "Train Epoch: 200 \tLoss: 0.002063\n",
      "Train Epoch: 201 \tLoss: 0.001825\n",
      "Train Epoch: 202 \tLoss: 0.002779\n",
      "Train Epoch: 203 \tLoss: 0.001661\n",
      "Train Epoch: 204 \tLoss: 0.001973\n",
      "Train Epoch: 205 \tLoss: 0.001669\n",
      "Train Epoch: 206 \tLoss: 0.002601\n",
      "Train Epoch: 207 \tLoss: 0.002277\n",
      "Train Epoch: 208 \tLoss: 0.001722\n",
      "Train Epoch: 209 \tLoss: 0.002582\n",
      "Train Epoch: 210 \tLoss: 0.002383\n",
      "Train Epoch: 211 \tLoss: 0.001978\n",
      "Train Epoch: 212 \tLoss: 0.002664\n",
      "Train Epoch: 213 \tLoss: 0.001827\n",
      "Train Epoch: 214 \tLoss: 0.002133\n",
      "Train Epoch: 215 \tLoss: 0.001707\n",
      "Train Epoch: 216 \tLoss: 0.002292\n",
      "Train Epoch: 217 \tLoss: 0.001656\n",
      "Train Epoch: 218 \tLoss: 0.002139\n",
      "Train Epoch: 219 \tLoss: 0.002222\n",
      "Train Epoch: 220 \tLoss: 0.002000\n",
      "Train Epoch: 221 \tLoss: 0.001665\n",
      "Train Epoch: 222 \tLoss: 0.001684\n",
      "Train Epoch: 223 \tLoss: 0.001986\n",
      "Train Epoch: 224 \tLoss: 0.002135\n",
      "Train Epoch: 225 \tLoss: 0.002655\n",
      "Train Epoch: 226 \tLoss: 0.001924\n",
      "Train Epoch: 227 \tLoss: 0.002597\n",
      "Train Epoch: 228 \tLoss: 0.001837\n",
      "Train Epoch: 229 \tLoss: 0.001879\n",
      "Train Epoch: 230 \tLoss: 0.001724\n",
      "Train Epoch: 231 \tLoss: 0.001734\n",
      "Train Epoch: 232 \tLoss: 0.001861\n",
      "Train Epoch: 233 \tLoss: 0.001998\n",
      "Train Epoch: 234 \tLoss: 0.001124\n",
      "Train Epoch: 235 \tLoss: 0.002225\n",
      "Train Epoch: 236 \tLoss: 0.001523\n",
      "Train Epoch: 237 \tLoss: 0.001647\n",
      "Train Epoch: 238 \tLoss: 0.001743\n",
      "Train Epoch: 239 \tLoss: 0.002144\n",
      "Train Epoch: 240 \tLoss: 0.001819\n",
      "Train Epoch: 241 \tLoss: 0.001973\n",
      "Train Epoch: 242 \tLoss: 0.002569\n",
      "Train Epoch: 243 \tLoss: 0.001914\n",
      "Train Epoch: 244 \tLoss: 0.001803\n",
      "Train Epoch: 245 \tLoss: 0.001870\n",
      "Train Epoch: 246 \tLoss: 0.002062\n",
      "Train Epoch: 247 \tLoss: 0.001948\n",
      "Train Epoch: 248 \tLoss: 0.001910\n",
      "Train Epoch: 249 \tLoss: 0.001820\n",
      "Train Epoch: 250 \tLoss: 0.001959\n",
      "Train Epoch: 251 \tLoss: 0.001962\n",
      "Train Epoch: 252 \tLoss: 0.001593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 253 \tLoss: 0.001498\n",
      "Train Epoch: 254 \tLoss: 0.001893\n",
      "Train Epoch: 255 \tLoss: 0.001967\n",
      "Train Epoch: 256 \tLoss: 0.001470\n",
      "Train Epoch: 257 \tLoss: 0.002153\n",
      "Train Epoch: 258 \tLoss: 0.002411\n",
      "Train Epoch: 259 \tLoss: 0.002363\n",
      "Train Epoch: 260 \tLoss: 0.001927\n",
      "Train Epoch: 261 \tLoss: 0.002259\n",
      "Train Epoch: 262 \tLoss: 0.001911\n",
      "Train Epoch: 263 \tLoss: 0.002537\n",
      "Train Epoch: 264 \tLoss: 0.001705\n",
      "Train Epoch: 265 \tLoss: 0.001626\n",
      "Train Epoch: 266 \tLoss: 0.001908\n",
      "Train Epoch: 267 \tLoss: 0.001799\n",
      "Train Epoch: 268 \tLoss: 0.001751\n",
      "Train Epoch: 269 \tLoss: 0.001655\n",
      "Train Epoch: 270 \tLoss: 0.001692\n",
      "Train Epoch: 271 \tLoss: 0.001777\n",
      "Train Epoch: 272 \tLoss: 0.002127\n",
      "Train Epoch: 273 \tLoss: 0.001492\n",
      "Train Epoch: 274 \tLoss: 0.002116\n",
      "Train Epoch: 275 \tLoss: 0.001359\n",
      "Train Epoch: 276 \tLoss: 0.002213\n",
      "Train Epoch: 277 \tLoss: 0.002017\n",
      "Train Epoch: 278 \tLoss: 0.001671\n",
      "Train Epoch: 279 \tLoss: 0.001821\n",
      "Train Epoch: 280 \tLoss: 0.002522\n",
      "Train Epoch: 281 \tLoss: 0.001734\n",
      "Train Epoch: 282 \tLoss: 0.001981\n",
      "Train Epoch: 283 \tLoss: 0.001467\n",
      "Train Epoch: 284 \tLoss: 0.002127\n",
      "Train Epoch: 285 \tLoss: 0.001626\n",
      "Train Epoch: 286 \tLoss: 0.002318\n",
      "Train Epoch: 287 \tLoss: 0.001900\n",
      "Train Epoch: 288 \tLoss: 0.001464\n",
      "Train Epoch: 289 \tLoss: 0.001994\n",
      "Train Epoch: 290 \tLoss: 0.001881\n",
      "Train Epoch: 291 \tLoss: 0.001642\n",
      "Train Epoch: 292 \tLoss: 0.002000\n",
      "Train Epoch: 293 \tLoss: 0.002390\n",
      "Train Epoch: 294 \tLoss: 0.001848\n",
      "Train Epoch: 295 \tLoss: 0.002578\n",
      "Train Epoch: 296 \tLoss: 0.001313\n",
      "Train Epoch: 297 \tLoss: 0.002035\n",
      "Train Epoch: 298 \tLoss: 0.001950\n",
      "Train Epoch: 299 \tLoss: 0.001959\n",
      "Train Epoch: 300 \tLoss: 0.001746\n"
     ]
    }
   ],
   "source": [
    "model = Regressor().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# train\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(log_interval, model, train_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4653, Accuracy: 15148/17652 (86%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_losses = []\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "criterion =  nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "    predictions = model(X_val)\n",
    "    test_loss += criterion(predictions, y_val)\n",
    "    pred = predictions.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "    correct += pred.eq(y_val.view_as(pred)).sum().item()\n",
    "\n",
    "test_losses.append(test_loss)\n",
    "\n",
    "test_loss /= len(X_val)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct,\n",
    "                                                                             len(X_val),\n",
    "                                                                             100. * correct / len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "savePath = \"/home/sungeun/deep-high-resolution-net.pytorch-master/demo/submission_1024.pth\"\n",
    "torch.save(model.state_dict(), savePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision / Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.844e+03 6.570e+02 5.200e+01 0.000e+00]\n",
      " [3.480e+02 2.682e+03 4.540e+02 0.000e+00]\n",
      " [9.000e+00 2.850e+02 2.696e+03 1.410e+02]\n",
      " [0.000e+00 3.000e+00 5.550e+02 1.926e+03]]\n"
     ]
    }
   ],
   "source": [
    "confusionMatrix = np.zeros((4,4))\n",
    "for i, y in enumerate(y_val):\n",
    "    confusionMatrix[y, pred[i]] += 1\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision = avg(TP/TP+FP)\n",
    "#Recall = avg(TP/TP+FN)\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "for i in range(4):\n",
    "    TP = confusionMatrix[i, i]\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for j in range(4):\n",
    "        if(i!=j):\n",
    "            FP += confusionMatrix[j, i]\n",
    "            FN += confusionMatrix[i, j]\n",
    "            pre = TP / (TP+FP)\n",
    "            re = TP / (TP+FN)\n",
    "    precisions.append(pre)\n",
    "    recalls.append(re)\n",
    "\n",
    "avgPre = sum(precisions, 0.0)/len(precisions)\n",
    "avgRe = sum(recalls, 0.0)/len(recalls)\n",
    "F1_Score = 2*(avgPre*avgRe)/(avgPre+avgRe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision :  0.9564687233264236\n",
      "Recall :  0.9171051093183679\n",
      "\n",
      "\n",
      "Precision :  0.739454094292804\n",
      "Recall :  0.769804822043628\n",
      "\n",
      "\n",
      "Precision :  0.7175938248602608\n",
      "Recall :  0.861066751836474\n",
      "\n",
      "\n",
      "Precision :  0.9317851959361393\n",
      "Recall :  0.7753623188405797\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "AvgPre :  0.8363254596039069\n",
      "AvgRe :  0.8308347505097624\n",
      "------------------------------------\n",
      "F1 score :  0.8335710633684073\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print('Precision : ', precisions[i])\n",
    "    print('Recall : ', recalls[i])\n",
    "    print('\\n')\n",
    "print('------------------------------------')\n",
    "print('AvgPre : ', avgPre)\n",
    "print('AvgRe : ', avgRe)\n",
    "print('------------------------------------')\n",
    "print('F1 score : ', F1_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
